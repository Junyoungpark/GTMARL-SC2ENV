{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A short tutorial for SMAC\n",
    "\n",
    "Author: Junyoung Park (Junyoungpark@kaist.ac.kr)  \n",
    "Original Implementation is brought from [this link](https://github.com/oxwhirl/smac/blob/master/smac/examples/random_agents.py).\n",
    "\n",
    "* The code may not run properly on Jupyter notebook. The runnable script is prepared in `random_agnet.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smac.env import StarCraft2Env # Import SMAC env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a game runner\n",
    "\n",
    "`StarCraft2Env` is a RL friendly interface of Starcraf II (SC2) and python. Developed by the SMAC team. Among the various input parameters of `StarCraft2Env`, we will tweak few parameters of followings:\n",
    "* __window_size_x__, __window_size_y__ (Floats) : Two parameters will specify the window size of SC2 launcher. We use 1920/3, 1080/3, (1/3 of Full HD resolution) respectively.\n",
    "* __difficulty__ (string casted Integers) : The parameter specifies the difficulty of SC2 environment. You can tweak this paramter when you train your model. In test, you will going to fix this value as \"4\" - MediumHard level of Blizzard AI.  \n",
    "* __map_name__ (string): The parameter specifies scenario of environment. We will use \"8m\". i.e.) 8 __Marines__ vs. 8 __Marines__ scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Marine](../img/Marine.gif)\n",
    "Marine is a multi-purpose ranged unit of Terran race in SC2. In this project, the marines are only capable to stop, move, and attack enemy. No upgrades are applied to the Marines including damage, defense, and combat shield. Also, we assume that the marines will not use stimpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to tune your reward\n",
    "\n",
    "Reward is one of the fundamental tuning knob of RL. you can tweak the reward of `StarCraft2Env` by passing reward related arguments to the constructor of `StarCraft2Env`. Here's the list of parameters that you may want to tune:\n",
    "* __reward_sparse__ (bool) : Recieve 1/-1 reward for winning/losing an episode. If true, other reward parameters are ignored.\n",
    "* __reward_death_value__ (float) : The amount of reward recieved for killing an enemy unit (default is 10)\n",
    "* __reward_win__ / __reward_lose__ (float): The reward for winning/losing in an episode (default is 200/0.)\n",
    "\n",
    "practical tips:\n",
    "* setting up sparse reward as __true__ can be benefical to suppress unexpected behavior of the learning agents. Even we deliverately tune the rewards, the agent may behaves unexpectedly when it comes to win the episodes when your agent recieves reward something other than 1/-1 rewards when they win/lose.\n",
    "\n",
    "* beware of zero penalty when the agents lose in an episode. Without properly penalizing the agents, you will get only-attack-knowing agent in the end.\n",
    "\n",
    "\n",
    "You can fine the full documentation of the `StarCraft2Env` constructor from [here](https://github.com/oxwhirl/smac/blob/master/smac/env/starcraft2/starcraft2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StarCraft2Env(map_name=\"8m\",\n",
    "                    window_size_x=1920/3,\n",
    "                    window_size_y=1080/3,\n",
    "                    reward_sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving meta information about the SC2 env\n",
    "\n",
    "`StarCraft2Env` supports a function that might be used for specifying the shape of inputs and other parameters of your model. You can instantly access to the meta information of `StarCraft2Env` by calling `env.get_env_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action of each agent: 14\n",
      "How many agent will you control (at beginning): 8 \n"
     ]
    }
   ],
   "source": [
    "env_info = env.get_env_info()\n",
    "n_actions = env_info[\"n_actions\"]\n",
    "n_agents = env_info[\"n_agents\"]\n",
    "\n",
    "print(\"The action of each agent: {}\".format(n_actions))\n",
    "print(\"How many agent will you control (at beginning): {} \".format(n_agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 50 # Number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing Neural networks (or any other tensor-ouputed learning method) with SC2\n",
    "\n",
    "`avail_actions` gives you the agent specific action space. the length of `avail_actions` will vary depending on the scenario. In general (unless you made a modification on the `StarCraft2Env`), the dimension of actions space is\n",
    "$$\\text{Dimension of action space} = 1+1+4+ \\text{number of enemy agents}$$\n",
    "\n",
    "* where the first 1 is for __no_operation__ which is doing nothing\n",
    "* where the second 1 is for __stop__ (in SC2, stop will cease everything what agent does.)\n",
    "* where the third 4 is for __move__ (depedning on the actions, the agents will move to the North, South, East, West). Note the amount of moving is determined by `StarCraft2Env`\n",
    "* where the fourth $\\text{Number of enemy agents}$ is for denoting which enemy will attack by the agent.\n",
    "\n",
    "For instance, if the `avail_actions` of a agent is [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "then we can infer that the agent __can__ do 'stop', 'move North', 'move South', 'move East', and 'move West' and __cannot__ do 'no-op', 'attack enemies'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward in episode 0 = 1.6875\n",
      "Total reward in episode 1 = 2.25\n",
      "Total reward in episode 2 = 1.6875\n",
      "Total reward in episode 3 = 2.0625\n",
      "Total reward in episode 4 = 1.5\n",
      "Total reward in episode 5 = 1.3125\n",
      "Total reward in episode 6 = 2.0625\n",
      "Total reward in episode 7 = 1.5\n",
      "Total reward in episode 8 = 1.6875\n",
      "Total reward in episode 9 = 1.875\n",
      "Total reward in episode 10 = 1.125\n",
      "Total reward in episode 11 = 2.25\n",
      "Total reward in episode 12 = 1.6875\n",
      "Total reward in episode 13 = 1.6875\n",
      "Total reward in episode 14 = 1.6875\n",
      "Total reward in episode 15 = 2.0625\n",
      "Total reward in episode 16 = 1.6875\n",
      "Total reward in episode 17 = 1.875\n",
      "Total reward in episode 18 = 2.4375\n",
      "Total reward in episode 19 = 1.875\n",
      "Total reward in episode 20 = 1.5\n",
      "Total reward in episode 21 = 2.4375\n",
      "Total reward in episode 22 = 2.4375\n",
      "Total reward in episode 23 = 1.125\n",
      "Total reward in episode 24 = 2.0625\n",
      "Total reward in episode 25 = 2.0625\n",
      "Total reward in episode 26 = 1.3125\n",
      "Total reward in episode 27 = 1.5\n",
      "Total reward in episode 28 = 1.5\n",
      "Total reward in episode 29 = 2.4375\n",
      "Total reward in episode 30 = 1.5\n",
      "Total reward in episode 31 = 2.625\n",
      "Total reward in episode 32 = 2.8125\n",
      "Total reward in episode 33 = 1.6875\n",
      "Total reward in episode 34 = 2.4375\n",
      "Total reward in episode 35 = 1.5\n",
      "Total reward in episode 36 = 1.6875\n",
      "Total reward in episode 37 = 1.875\n",
      "Total reward in episode 38 = 2.0625\n",
      "Total reward in episode 39 = 2.0625\n",
      "Total reward in episode 40 = 1.125\n",
      "Total reward in episode 41 = 2.0625\n",
      "Total reward in episode 42 = 2.0625\n",
      "Total reward in episode 43 = 1.125\n",
      "Total reward in episode 44 = 1.5\n",
      "Total reward in episode 45 = 1.875\n",
      "Total reward in episode 46 = 1.125\n",
      "Total reward in episode 47 = 1.5\n",
      "Total reward in episode 48 = 2.4375\n",
      "Total reward in episode 49 = 1.3125\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_episodes):\n",
    "    env.reset() # Reset everything to make clean start.\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not terminated:\n",
    "        obs = env.get_obs() # give you a list that contains each agent's observaion\n",
    "        state = env.get_state() # The global state reserved for centralized exectuion\n",
    "\n",
    "        actions = []\n",
    "        for agent_id in range(n_agents):\n",
    "            \n",
    "            avail_actions = env.get_avail_agent_actions(agent_id) # give you per-agent actions space mask)\n",
    "            avail_actions_ind = np.nonzero(avail_actions)[0]  \n",
    "            action = np.random.choice(avail_actions_ind)\n",
    "            actions.append(action)\n",
    "\n",
    "        reward, terminated, _ = env.step(actions)\n",
    "        episode_reward += reward\n",
    "    print(\"Total reward in episode {} = {}\".format(e, episode_reward))\n",
    "\n",
    "env.close() # this will close all connections of SC2 (+ SC2 launcher). make sure to call after you run entire code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
